# -*- coding: utf-8 -*-
"""Mathew_Sheethal_634Option1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1vne05liWb6hsMCkkRltfapYEh348aSdq
"""

# Commented out IPython magic to ensure Python compatibility.
from mlxtend.plotting import plot_decision_regions
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import warnings
warnings.filterwarnings('ignore')
# %matplotlib inline

#read dataset
attributes = pd.read_csv('diabetes.csv')
#printing first 5 lines of dataset
attributes.head(5)

#idetify anomalies using describe()
attributes.describe()

#Cleaning up anomalies in data and handling null data
diabetes_data = attributes.copy(deep = True)
diabetes_data[['Glucose','BloodPressure','SkinThickness','Insulin','BMI']] = diabetes_data[['Glucose','BloodPressure','SkinThickness','Insulin','BMI']].replace(0,np.NaN)
diabetes_data['Glucose'].fillna(diabetes_data['Glucose'].mean(), inplace = True)
diabetes_data['BloodPressure'].fillna(diabetes_data['BloodPressure'].mean(), inplace = True)
diabetes_data['SkinThickness'].fillna(diabetes_data['SkinThickness'].median(), inplace = True)
diabetes_data['Insulin'].fillna(diabetes_data['Insulin'].median(), inplace = True)
diabetes_data['BMI'].fillna(diabetes_data['BMI'].median(), inplace = True)

#split data into 25%test
from sklearn.model_selection import train_test_split
train_attributes, test_attributes = train_test_split(attributes, test_size=0.25, random_state=0, stratify=attributes['Outcome'])

trainAttr = train_attributes[train_attributes.columns[:8]]
testAttr = test_attributes[test_attributes.columns[:8]]
trainLabel = train_attributes['Outcome']
testLabel = test_attributes['Outcome']

#RandomForest Model Classification
from sklearn.ensemble import RandomForestClassifier
from sklearn import metrics
from sklearn.metrics import accuracy_score
randModel = RandomForestClassifier(n_estimators=100,random_state=0) #n_estimators is number of trees in forest
randModel.fit(train_attributes, trainLabel)
predictedTest = randModel.predict(test_attributes)

print('The accuracy Score is:\n',metrics.accuracy_score(predictedTest,testLabel))

#The model seem to have an accuracy value of 64.06%

#reduce the depth of the tree to 3 levels, instead of the original 100 branch tree
from sklearn.ensemble import RandomForestRegressor
from sklearn.tree import export_graphviz
import pydot
rf_small = RandomForestRegressor(n_estimators=10, max_depth = 3)
rf_small.fit(train_attributes, trainLabel)
# Extract the small tree
tree_small = rf_small.estimators_[5]
# Saving feature names for later use
attr_list = list(attributes.columns)
# Save the tree as a png image
export_graphviz(tree_small, out_file = 'small_tree.dot', feature_names= attr_list, rounded = True, precision = 1)
(graph, ) = pydot.graph_from_dot_file('small_tree.dot')
graph.write_png('small_tree.png');

from sklearn.metrics import confusion_matrix

print('\n \n The RandomForest confusion matrix: \n', confusion_matrix(predictedTest, testLabel))
print('\n\n The metrics RandomForest classification report:\n ', metrics.classification_report(predictedTest, testLabel))

#Area under curve
prob = randModel.predict_proba(test_attributes)
prob = prob[:, 1]

auc = metrics.roc_auc_score(testLabel, prob)
print('AUC score is: ',auc)

#receiver operating characteristic
from sklearn.metrics import plot_roc_curve
from sklearn.svm import SVC
ax = plt.gca()
randModel_disp = plot_roc_curve(randModel, test_attributes, testLabel, ax=ax, alpha=0.8)
svc = SVC(random_state=42)
svc.fit(test_attributes, testLabel)
svc_disp = plot_roc_curve(svc, test_attributes, testLabel)
svc_disp.plot(ax=ax, alpha=0.8)

#plt.show()

"""KNN Algorithm 

"""

#normalization range 0 and 1, perplexed as to why some data is -2.6, or 1.5
from sklearn.preprocessing import StandardScaler
scale_attr = StandardScaler()
attr = pd.DataFrame(scale_attr.fit_transform(diabetes_data.drop(["Outcome"],axis=1),),
                 columns=['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age'])

attributes_new= attributes.drop('Outcome', axis = 1)
print(scale_attr.transform(attributes_new))
attr.head()

#from sklearn.preprocessing import StandardScale
#normalize data to calculate euclidean distance for knn algorithm
#if the weightage is not correctly calculated the euclidean distance will be skewed for some data points
from sklearn import preprocessing
a = np.array(attributes)
scaler = preprocessing.StandardScaler()
#transform
standrd = scaler.fit_transform(a)
standrd

ylabel = diabetes_data.Outcome
ylabel

#Test, Train, Validate after splitting data
#split data with test size of 1/3rd of overal data size
from sklearn.model_selection import train_test_split
Attr_train, Attr_test, label_train, label_test = train_test_split(attr,ylabel,test_size=1/3, random_state=42, stratify=ylabel)

#use knn classifier library, datascientist have to provide the value of k
from sklearn.neighbors import KNeighborsClassifier
test_calc = []
train_calc = []
for i in range(1,7):
  knn = KNeighborsClassifier(i)
  knn.fit(Attr_train, label_train)

#need to graphically represent growth
  train_calc.append(knn.score(Attr_train, label_train))
  test_calc.append(knn.score(Attr_test, label_test))
  test_calc

#score from using same data point for testing and training will be 100
#below you will find score from testing split data points with 1/3 set aside for testing
max_dist_test = max(test_calc)
i_test_calc = [i for i, v in enumerate(test_calc) if v == max_dist_test]
print('Max test score {} % and k = {}'.format(max_dist_test*100, list(map(lambda x: x+1, i_test_calc))))

#plot the knn results of test vs train
#below you find mapping insulin, glucose level to outcome of diabetes
import seaborn as sns
sns.set()
sns.lmplot(x="Glucose", y = "Insulin", hue = "Outcome", data=attributes )

p = sns.lineplot(range(1,7), train_calc, marker='*', label='Train Calculated')
p = sns.lineplot(range(1,7), test_calc, marker='o', label='Test Calculated')

#When enumerating through test data and calculating distance at each i, it can be found that k=3(3 classification) has the best result
knn = KNeighborsClassifier(11)
knn.fit(Attr_train, label_train) #run the model of train data
knn.score(Attr_test, label_test)#test the model on test data allocated(1/3 size)

#plot decision boundaries
#add number of attributes you have to filler_feature when plotting decision boundaries
value = 3
width = 3
plot_decision_regions(attr.values, ylabel.values, clf=knn, legend=2,
                      filler_feature_values={2: value, 3: value, 4:value, 5: value, 6:value, 7:value},
                      filler_feature_ranges={2: width, 3: width, 4: width, 5: width, 6: width, 7:width},
                      X_highlight=test_attributes.values)
plt.title('KNN with Diabetes Data')
plt.show()

#confusion matrix
from sklearn.metrics import confusion_matrix
label_pred = knn.predict(Attr_test)
confusion_matrix(label_test, label_pred)
pd.crosstab(label_test, label_pred, rownames=['Actual'], colnames=['Predicted'], margins=True)

from sklearn.metrics import confusion_matrix

print('\n \n The KNN confusion matrix: \n', confusion_matrix(label_test, label_pred))
print('\n\n The metrics KNN classification report:\n ', metrics.classification_report(label_test, label_pred))

#Area under curve
predictLabel_prob = knn.predict_proba(Attr_test)[:,1]

auc = metrics.roc_auc_score(label_test, predictLabel_prob)
print('AUC score is: ',auc)

#receiver operating characteristic
ax = plt.gca()
knnModel_disp = plot_roc_curve(knn, Attr_test, label_test, ax=ax, alpha=0.8)
svc = SVC(random_state=42)
svc.fit(Attr_test, label_test)
svc_disp = plot_roc_curve(svc, Attr_test, label_test)
svc_disp.plot(ax=ax, alpha=0.8)

"""DeepLearning Model : LSTM

"""

from tensorflow.keras import Sequential
from keras.layers import Dense#, LSTM, GRU, SimpleRNN
from keras.layers import LSTM
from sklearn.preprocessing import MinMaxScaler

lstm_dataset = np.array(attr)
lstm_dataset.reshape(-1,1) #normalizing
plt.plot(lstm_dataset) #visualize

#machine learning works better if data is scaled
scaler = MinMaxScaler()
lstm_dataset = scaler.fit_transform(lstm_dataset)

#split data into training and testing
test_size = len(lstm_dataset) - int(len(lstm_dataset) * 0.75) #total size of dataset minus 75% training set size
trainingset=lstm_dataset[:int(len(lstm_dataset) * 0.75),:]
testingset=lstm_dataset[int(len(lstm_dataset) * 0.75):142,:]
def getdata(data,lookback):
    X,Y=[],[]
    for i in range(len(data)-lookback-1):
        X.append(data[i:i+lookback,0])
        Y.append(data[i+lookback,0])
    return np.array(X),np.array(Y).reshape(-1,1)
lookback=1
X_train,y_train=getdata(trainingset,lookback)
X_test,y_test=getdata(testingset,lookback)
X_train=X_train.reshape(X_train.shape[0],X_train.shape[1],1)
#X_test=X_test.reshape(X_test.shape[0],X_test.shape[1],1)

deepLearnModel = Sequential()
deepLearnModel.add(LSTM(5,input_shape=(1,lookback))) #5 neurons of first layer of neural network
deepLearnModel.add(Dense(1))
deepLearnModel.compile(loss='mean_squared_error', optimizer='adam',run_eagerly=True) #To avoid unexpected result of deepLearnModel.predict() function add run_eagerly
deepLearnModel.summary()

#model is ready for training

deepLearnModel.build(input_shape=(5, 42))#if you dont build the function before fit it results in the following error: "ValueError: Creating variables on a non-first call to a function decorated with tf.function."
deepLearnModel.fit(X_train, y_train, epochs=10, batch_size=1)

y_pred=deepLearnModel.predict(X_test)
y_test=deepLearnModel.inverse_transform(y_test)
y_pred=scaler.inverse_transform(y_pred)